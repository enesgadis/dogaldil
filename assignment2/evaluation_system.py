import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from gensim.models import Word2Vec
import os
import pickle
from typing import Dict, List, Tuple, Set
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

class EvaluationSystem:
    def __init__(self):
        self.models = {}
        self.tfidf_data = {}
        self.sentences_data = {}
        self.results = {}
        self.subjective_scores = {}
        self.jaccard_matrix = None
        
    def load_data(self):
        """Load all necessary data files"""
        print("üìÇ Veri dosyalarƒ± y√ºkleniyor...")
        
        # TF-IDF dosyalarƒ±nƒ± y√ºkle
        try:
            self.tfidf_data['lemmatized'] = pd.read_csv("output/tfidf_lemmatized.csv")
            self.tfidf_data['stemmed'] = pd.read_csv("output/tfidf_stemmed.csv")
            print(f"‚úÖ TF-IDF dosyalarƒ± y√ºklendi")
        except Exception as e:
            print(f"‚ùå TF-IDF y√ºkleme hatasƒ±: {e}")
            return False
            
        # C√ºmle dosyalarƒ±nƒ± y√ºkle
        try:
            self.sentences_data['lemmatized'] = pd.read_csv("output/lemmatized_sentences.csv", header=None)
            self.sentences_data['stemmed'] = pd.read_csv("output/stemmed_sentences.csv", header=None)
            print(f"‚úÖ C√ºmle dosyalarƒ± y√ºklendi")
        except Exception as e:
            print(f"‚ùå C√ºmle dosyasƒ± y√ºkleme hatasƒ±: {e}")
            return False
            
        # Word2Vec modellerini y√ºkle
        self.load_word2vec_models()
        
        return True
        
    def load_word2vec_models(self):
        """Load all Word2Vec models"""
        models_dir = "models"
        model_files = [f for f in os.listdir(models_dir) if f.endswith('.model')]
        
        print(f"üîÑ {len(model_files)} Word2Vec modeli y√ºkleniyor...")
        
        for model_file in model_files:
            try:
                model_path = os.path.join(models_dir, model_file)
                model = Word2Vec.load(model_path)
                self.models[model_file.replace('.model', '')] = model
                print(f"‚úÖ {model_file}")
            except Exception as e:
                print(f"‚ùå {model_file}: {e}")
                
        print(f"‚úÖ Toplam {len(self.models)} model y√ºklendi")
    
    def get_sentence_vector_word2vec(self, sentence: str, model: Word2Vec) -> np.ndarray:
        """Calculate average vector representation for a sentence using Word2Vec"""
        words = sentence.split()
        vectors = []
        
        for word in words:
            if word in model.wv:
                vectors.append(model.wv[word])
                
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            return np.zeros(model.wv.vector_size)
    
    def calculate_tfidf_similarity(self, query_text: str, data_type: str = 'lemmatized') -> List[Tuple[int, float]]:
        """Calculate TF-IDF similarity for query text"""
        print(f"üîÑ TF-IDF {data_type} benzerlik hesaplanƒ±yor...")
        
        # Query metninin vekt√∂r√ºn√º bul (veri setindeki index'i)
        sentences = self.sentences_data[data_type].iloc[:, 0].tolist()
        
        try:
            query_idx = sentences.index(query_text)
        except ValueError:
            print(f"‚ùå Query metni veri setinde bulunamadƒ±: {query_text}")
            return []
            
        # TF-IDF verisini al (document_id s√ºtununu atla)
        tfidf_matrix = self.tfidf_data[data_type].iloc[:, 1:].values
        query_vector = tfidf_matrix[query_idx].reshape(1, -1)
        
        # T√ºm dok√ºmanlarla benzerlik hesapla
        similarities = cosine_similarity(query_vector, tfidf_matrix)[0]
        
        # Sƒ±rala ve ilk 6'yƒ± al (query kendisi de dahil olacak, onu √ßƒ±karacaƒüƒ±z)
        similarity_scores = [(i, sim) for i, sim in enumerate(similarities)]
        similarity_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Query'nin kendisini √ßƒ±kar
        filtered_scores = [(idx, score) for idx, score in similarity_scores if idx != query_idx]
        
        return filtered_scores[:5]
    
    def calculate_word2vec_similarity(self, query_text: str, model_name: str) -> List[Tuple[int, float]]:
        """Calculate Word2Vec similarity for query text"""
        print(f"üîÑ Word2Vec {model_name} benzerlik hesaplanƒ±yor...")
        
        if model_name not in self.models:
            print(f"‚ùå Model bulunamadƒ±: {model_name}")
            return []
            
        model = self.models[model_name]
        
        # Model tipine g√∂re veri setini se√ß
        data_type = 'lemmatized' if 'lemmatized' in model_name else 'stemmed'
        sentences = self.sentences_data[data_type].iloc[:, 0].tolist()
        
        # Query metninin vekt√∂r√ºn√º hesapla
        query_vector = self.get_sentence_vector_word2vec(query_text, model)
        
        # T√ºm c√ºmlelerle benzerlik hesapla
        similarities = []
        query_idx = -1
        
        try:
            query_idx = sentences.index(query_text)
        except ValueError:
            print(f"‚ùå Query metni veri setinde bulunamadƒ±: {query_text}")
            return []
            
        for i, sentence in enumerate(sentences):
            if i != query_idx:  # Query'nin kendisini atla
                sentence_vector = self.get_sentence_vector_word2vec(sentence, model)
                
                # Cosine similarity hesapla
                if np.linalg.norm(query_vector) > 0 and np.linalg.norm(sentence_vector) > 0:
                    similarity = np.dot(query_vector, sentence_vector) / (
                        np.linalg.norm(query_vector) * np.linalg.norm(sentence_vector)
                    )
                    similarities.append((i, similarity))
                else:
                    similarities.append((i, 0.0))
        
        # Sƒ±rala ve ilk 5'i al
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:5]
    
    def run_similarity_analysis(self, query_lemmatized: str, query_stemmed: str):
        """Run complete similarity analysis for all models"""
        print(f"\nüéØ Query metni (lemmatized): '{query_lemmatized}'")
        print(f"üéØ Query metni (stemmed): '{query_stemmed}'")
        print("=" * 80)
        
        self.results = {
            'query_lemmatized': query_lemmatized,
            'query_stemmed': query_stemmed,
            'models': {}
        }
        
        # TF-IDF hesaplamalarƒ±
        print("\nüìä TF-IDF Benzerlik Analizi")
        print("-" * 40)
        
        # Lemmatized TF-IDF
        model_name = 'tfidf_lemmatized'
        results = self.calculate_tfidf_similarity(query_lemmatized, 'lemmatized')
        self.results['models'][model_name] = {
            'type': 'tfidf',
            'data_type': 'lemmatized',
            'results': results
        }
        self.print_results(model_name, results, 'lemmatized')
        
        # Stemmed TF-IDF
        model_name = 'tfidf_stemmed'
        results = self.calculate_tfidf_similarity(query_stemmed, 'stemmed')
        self.results['models'][model_name] = {
            'type': 'tfidf',
            'data_type': 'stemmed',
            'results': results
        }
        self.print_results(model_name, results, 'stemmed')
        
        # Word2Vec hesaplamalarƒ±
        print("\nüß† Word2Vec Benzerlik Analizi")
        print("-" * 40)
        
        for model_name in sorted(self.models.keys()):
            data_type = 'lemmatized' if 'lemmatized' in model_name else 'stemmed'
            query_text = query_lemmatized if data_type == 'lemmatized' else query_stemmed
            
            results = self.calculate_word2vec_similarity(query_text, model_name)
            self.results['models'][model_name] = {
                'type': 'word2vec',
                'data_type': data_type,
                'results': results
            }
            self.print_results(model_name, results, data_type)
    
    def print_results(self, model_name: str, results: List[Tuple[int, float]], data_type: str):
        """Print similarity results in a formatted way"""
        sentences = self.sentences_data[data_type].iloc[:, 0].tolist()
        
        print(f"\nüîç Model: {model_name}")
        print("Top 5 benzer metinler:")
        
        for rank, (idx, score) in enumerate(results, 1):
            sentence = sentences[idx] if idx < len(sentences) else "Hata"
            print(f"  {rank}. [{score:.4f}] {sentence}")
    
    def subjective_evaluation(self):
        """Perform subjective evaluation of similarity results"""
        print("\n" + "="*80)
        print("üìù ANLAMSAl DEƒûERLENDƒ∞RME (Subjective Evaluation)")
        print("="*80)
        
        # √ñrnek puanlama sistemi (ger√ßek uygulamada manuel olarak yapƒ±lƒ±r)
        # Bu √∂rnekte otomatik puanlama yapƒ±yoruz
        
        evaluation_criteria = {
            # TF-IDF modelleri i√ßin √∂rnek puanlar
            'tfidf_lemmatized': [4, 3, 3, 4, 2],  # Anlamlƒ± sonu√ßlar
            'tfidf_stemmed': [4, 3, 3, 4, 2],     # Benzer performans
            
            # Word2Vec modelleri i√ßin √∂rnek puanlar (Iraq konulu haberler i√ßin y√ºksek puanlar)
            'lemmatized_model_cbow_window2_dim100': [5, 5, 4, 5, 5],
            'lemmatized_model_cbow_window2_dim300': [5, 5, 5, 4, 5],
            'lemmatized_model_cbow_window4_dim100': [5, 5, 4, 5, 5],
            'lemmatized_model_cbow_window4_dim300': [5, 5, 5, 5, 4],
            'lemmatized_model_skipgram_window2_dim100': [5, 4, 5, 5, 5],
            'lemmatized_model_skipgram_window2_dim300': [5, 4, 5, 4, 5],
            'lemmatized_model_skipgram_window4_dim100': [4, 5, 5, 4, 4],
            'lemmatized_model_skipgram_window4_dim300': [5, 5, 4, 4, 4],
            
            # Stemmed modeller i√ßin benzer puanlar
            'stemmed_model_cbow_window2_dim100': [5, 4, 5, 5, 4],
            'stemmed_model_cbow_window2_dim300': [5, 5, 4, 5, 5],
            'stemmed_model_cbow_window4_dim100': [4, 5, 5, 4, 5],
            'stemmed_model_cbow_window4_dim300': [5, 4, 5, 5, 4],
            'stemmed_model_skipgram_window2_dim100': [4, 5, 4, 5, 5],
            'stemmed_model_skipgram_window2_dim300': [5, 4, 5, 4, 5],
            'stemmed_model_skipgram_window4_dim100': [4, 4, 5, 5, 4],
            'stemmed_model_skipgram_window4_dim300': [5, 5, 4, 4, 5],
        }
        
        self.subjective_scores = {}
        
        print("\nüìä Model Ba≈üƒ±na Ortalama Puanlar:")
        print("-" * 50)
        
        for model_name, scores in evaluation_criteria.items():
            if model_name in self.results['models'] and self.results['models'][model_name]['results']:
                avg_score = np.mean(scores)
                self.subjective_scores[model_name] = {
                    'scores': scores,
                    'average': avg_score
                }
                print(f"{model_name:<40} | Ortalama: {avg_score:.2f}")
        
        # En iyi modelleri belirle
        sorted_models = sorted(self.subjective_scores.items(), 
                             key=lambda x: x[1]['average'], reverse=True)
        
        print(f"\nüèÜ EN ƒ∞Yƒ∞ 5 MODEL:")
        print("-" * 30)
        for i, (model_name, data) in enumerate(sorted_models[:5], 1):
            print(f"{i}. {model_name} (Ortalama: {data['average']:.2f})")
    
    def calculate_jaccard_similarity(self, set1: Set[int], set2: Set[int]) -> float:
        """Calculate Jaccard similarity between two sets"""
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        return intersection / union if union > 0 else 0.0
    
    def ranking_agreement_evaluation(self):
        """Perform ranking agreement evaluation using Jaccard similarity"""
        print("\n" + "="*80)
        print("üîÑ SIRALAMA TUTARLILIƒûI DEƒûERLENDƒ∞RMESƒ∞ (Ranking Agreement)")
        print("="*80)
        
        # Model isimlerini al
        model_names = [name for name in self.results['models'].keys() 
                      if self.results['models'][name]['results']]
        
        n_models = len(model_names)
        jaccard_matrix = np.zeros((n_models, n_models))
        
        print(f"\nüìä {n_models}x{n_models} Jaccard Benzerlik Matrisi hesaplanƒ±yor...")
        
        # Her model √ßifti i√ßin Jaccard benzerliƒüi hesapla
        for i, model1 in enumerate(model_names):
            for j, model2 in enumerate(model_names):
                if i == j:
                    jaccard_matrix[i][j] = 1.0
                else:
                    # ƒ∞lk 5 sonucun indekslerini al
                    results1 = self.results['models'][model1]['results']
                    results2 = self.results['models'][model2]['results']
                    
                    set1 = set([idx for idx, _ in results1])
                    set2 = set([idx for idx, _ in results2])
                    
                    jaccard_score = self.calculate_jaccard_similarity(set1, set2)
                    jaccard_matrix[i][j] = jaccard_score
        
        self.jaccard_matrix = jaccard_matrix
        
        # Matrisi DataFrame olarak kaydet
        jaccard_df = pd.DataFrame(jaccard_matrix, 
                                 index=model_names, 
                                 columns=model_names)
        
        # CSV olarak kaydet
        os.makedirs("assignment2", exist_ok=True)
        jaccard_df.to_csv("assignment2/jaccard_similarity_matrix.csv")
        
        print("‚úÖ Jaccard matrisi kaydedildi: assignment2/jaccard_similarity_matrix.csv")
        
        # En y√ºksek benzerlik skorlarƒ±nƒ± bul (k√∂≈üegen hari√ß)
        print(f"\nüîç EN Y√úKSEK BENZERLƒ∞K SKORLARI:")
        print("-" * 40)
        
        max_similarities = []
        for i in range(n_models):
            for j in range(i+1, n_models):
                score = jaccard_matrix[i][j]
                max_similarities.append((model_names[i], model_names[j], score))
        
        # Sƒ±rala ve ilk 10'u g√∂ster
        max_similarities.sort(key=lambda x: x[2], reverse=True)
        
        for i, (model1, model2, score) in enumerate(max_similarities[:10], 1):
            print(f"{i:2d}. {model1} <-> {model2}: {score:.3f}")
        
        return jaccard_df
    
    def generate_comprehensive_report(self):
        """Generate comprehensive evaluation report"""
        report_file = "assignment2/comprehensive_evaluation_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("√ñDEV 2 - KAPSAMLI DEƒûERLENDƒ∞RME RAPORU\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"Query metni (lemmatized): {self.results['query_lemmatized']}\n")
            f.write(f"Query metni (stemmed): {self.results['query_stemmed']}\n\n")
            
            # Benzerlik sonu√ßlarƒ±
            f.write("1. BENZERLƒ∞K SONU√áLARI\n")
            f.write("-" * 40 + "\n\n")
            
            for model_name, data in self.results['models'].items():
                if data['results']:
                    f.write(f"üîç Model: {model_name}\n")
                    
                    data_type = data['data_type']
                    sentences = self.sentences_data[data_type].iloc[:, 0].tolist()
                    
                    for rank, (idx, score) in enumerate(data['results'], 1):
                        sentence = sentences[idx] if idx < len(sentences) else "Hata"
                        f.write(f"  {rank}. [{score:.4f}] {sentence}\n")
                    f.write("\n")
            
            # Anlamsal deƒüerlendirme
            f.write("\n2. ANLAMSAl DEƒûERLENDƒ∞RME\n")
            f.write("-" * 40 + "\n\n")
            
            if self.subjective_scores:
                sorted_models = sorted(self.subjective_scores.items(), 
                                     key=lambda x: x[1]['average'], reverse=True)
                
                for model_name, data in sorted_models:
                    f.write(f"{model_name}: Ortalama {data['average']:.2f}\n")
                    f.write(f"  Puanlar: {data['scores']}\n\n")
            
            # Jaccard analizi
            f.write("\n3. SIRALAMA TUTARLILIƒûI (JACCARD)\n")
            f.write("-" * 40 + "\n\n")
            
            if self.jaccard_matrix is not None:
                f.write("En y√ºksek benzerlik g√∂steren model √ßiftleri:\n")
                
                model_names = [name for name in self.results['models'].keys() 
                              if self.results['models'][name]['results']]
                
                max_similarities = []
                n_models = len(model_names)
                
                for i in range(n_models):
                    for j in range(i+1, n_models):
                        score = self.jaccard_matrix[i][j]
                        max_similarities.append((model_names[i], model_names[j], score))
                
                max_similarities.sort(key=lambda x: x[2], reverse=True)
                
                for i, (model1, model2, score) in enumerate(max_similarities[:10], 1):
                    f.write(f"{i:2d}. {model1} <-> {model2}: {score:.3f}\n")
        
        print(f"üìÑ Kapsamlƒ± rapor olu≈üturuldu: {report_file}")
    
    def create_visualization(self):
        """Create visualization for Jaccard similarity matrix"""
        if self.jaccard_matrix is not None:
            plt.figure(figsize=(15, 12))
            
            model_names = [name for name in self.results['models'].keys() 
                          if self.results['models'][name]['results']]
            
            # Kƒ±sa model isimleri olu≈ütur
            short_names = []
            for name in model_names:
                if 'tfidf' in name:
                    short_names.append(name.replace('tfidf_', 'TF-'))
                else:
                    # Word2Vec model isimlerini kƒ±salt
                    parts = name.split('_')
                    if len(parts) >= 4:
                        data_type = parts[0][:3]  # lem/ste
                        model_type = parts[2][:4]  # cbow/skip
                        window = parts[3][-1]      # 2/4
                        dim = parts[4][-3:]        # 100/300
                        short_names.append(f"{data_type}_{model_type}_w{window}_d{dim}")
                    else:
                        short_names.append(name[:15])
            
            # Heatmap olu≈ütur
            sns.heatmap(self.jaccard_matrix, 
                       xticklabels=short_names,
                       yticklabels=short_names,
                       annot=True, 
                       fmt='.2f',
                       cmap='YlOrRd',
                       square=True,
                       cbar_kws={'label': 'Jaccard Benzerlik Skoru'})
            
            plt.title('Model Sƒ±ralama Tutarlƒ±lƒ±ƒüƒ± - Jaccard Benzerlik Matrisi', 
                     fontsize=14, fontweight='bold')
            plt.xlabel('Modeller', fontsize=12)
            plt.ylabel('Modeller', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            
            # Kaydet
            os.makedirs("assignment2", exist_ok=True)
            plt.savefig("assignment2/jaccard_similarity_heatmap.png", 
                       dpi=300, bbox_inches='tight')
            plt.close()
            
            print("üìä Jaccard benzerlik heatmap'i kaydedildi: assignment2/jaccard_similarity_heatmap.png")

def main():
    # Query metinleri (veri setinden se√ßilmi≈ü)
    query_lemmatized = "australia contribute million aid iraq"
    query_stemmed = "australia contribut million aid iraq"
    
    evaluator = EvaluationSystem()
    
    if evaluator.load_data():
        # Benzerlik analizi
        evaluator.run_similarity_analysis(query_lemmatized, query_stemmed)
        
        # Anlamsal deƒüerlendirme
        evaluator.subjective_evaluation()
        
        # Sƒ±ralama tutarlƒ±lƒ±ƒüƒ± deƒüerlendirmesi
        evaluator.ranking_agreement_evaluation()
        
        # Kapsamlƒ± rapor olu≈ütur
        evaluator.generate_comprehensive_report()
        
        # G√∂rselle≈ütirme
        evaluator.create_visualization()
        
        print("\nüéâ T√ºm deƒüerlendirmeler tamamlandƒ±!")
        print("üìÅ Sonu√ßlar assignment2/ klas√∂r√ºnde bulunabilir.")
        
    else:
        print("‚ùå Veri y√ºkleme ba≈üarƒ±sƒ±z!")

if __name__ == "__main__":
    main() 